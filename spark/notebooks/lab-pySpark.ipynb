{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD Spark et Op√©ration. \n",
    "\n",
    "Un RDD est la structure de base pour le traitement des donn√©es dans Apache Spark. il s'agit d'une collection : \n",
    "- R√©silent : si un noeud tombe, spark peut en reconstruire le RDD √† partir des transformations initiales grace aux DAG\n",
    "- Distribu√© -> Les donn√©es sont distribu√©es sur diff√©rents noeuds du cluster qui permet un traitement parall√®le. \n",
    "- Immutable -> Une fois cr√©e, un RDD ne peut pas √™tre modifi√©, mais on peut en g√©n√©rer un nouveau √† partir de transformation. \n",
    "- Layly Evaluated : les transformations appliqu√©es sur le RDD tel que map, filter ne sont pas appliqu√© qu'au moment d'une action comme : collect(), count() etc.\n",
    "- Partitionn√© -> Spark divise automatiquement les donn√©es en partitions pour am√©liorer l'efficacit√© du calcul. \n",
    "\n",
    "- Transformations (cr√©ent un nouveau RDD) : map(), filter(), flatMap(), reduceByKey(), etc.\n",
    "- Actions (d√©clenchent l‚Äôex√©cution des transformations) : collect(), count(), reduce(), saveAsTextFile(), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import random\n",
    "#cr√©er un SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession.builder.appName(\"RDD_to_DF\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red; font-size:1em; font-weight:bold\"> Diff√©rence entre SparkContext et SparkSession :</span> \n",
    "\n",
    "\n",
    "\n",
    "| üè∑Ô∏è Caract√©ristique  | ‚ö° SparkContext (`sc`) | üöÄ SparkSession (`spark`) |\n",
    "|-----------------|-----------------|------------------|\n",
    "| **D√©finition** | Point d‚Äôentr√©e original de Spark pour acc√©der au cluster. | API unifi√©e introduite dans Spark 2.0 qui regroupe `SparkContext`, `SQLContext` et `HiveContext`. |\n",
    "| **Utilisation principale** | Manipulation bas niveau des RDDs. | Manipulation des DataFrames, SQL, streaming et interactions avec Spark SQL. |\n",
    "| **Cr√©ation** | `sc = SparkContext.getOrCreate()` | `spark = SparkSession.builder.appName(\"App\").getOrCreate()` |\n",
    "| **Support SQL/DataFrame** | ‚ùå Non pris en charge directement. | ‚úÖ Prise en charge compl√®te (`spark.sql()`, `spark.read` etc.). |\n",
    "| **Acc√®s au SparkContext** | ‚úÖ C'est lui-m√™me le contexte principal. | ‚úÖ Peut acc√©der √† `SparkContext` via `spark.sparkContext`. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exemple de cr√©ation d'un RDD spark et de sa Transformation\n",
    "data = [1,2,3,4,5,6,7,8,9,10]\n",
    "rdd = sc.parallelize(data) # c√©tte op√©ration permet de cr√©er un RDD immutable. \n",
    "# Transformer les donn√©es : Nous allons les multiplier par 2\n",
    "rdd_multiplie = rdd.map(lambda x: x * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 8, 10, 12, 14, 16, 18, 20]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Action : R√©cup√©ration des r√©sultats. \n",
    "result = rdd_multiplie.collect()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lire le fichier .txt dans un RDD spark. \n",
    "text_rdd = sc.textFile('/home/jovyan/work/tears_in_rain.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I've seen things you people wouldn't believe. \",\n",
       " 'Attack ships on fire off the shoulder of Orion. ',\n",
       " 'I watched C-beams glitter in the dark near the Tannh√§user Gate. ']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Afficher les 3 premier √©l√©ments de notre RDD\n",
    "text_rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "le nombre d'√©l√©ment : 5\n",
      "les 10 premiers √©l√©ments  : [\"I've seen things you people wouldn't believe. \", 'Attack ships on fire off the shoulder of Orion. ', 'I watched C-beams glitter in the dark near the Tannh√§user Gate. ', 'All those moments will be lost in time, like tears in rain. ', 'Time to die.']\n"
     ]
    }
   ],
   "source": [
    "print(f\"le nombre d'√©l√©ment : {text_rdd.count()}\")\n",
    "print(f\"les 10 premiers √©l√©ments  : {text_rdd.take(10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliter les phrases en listes de Mots\n",
    "split_text = text_rdd.map(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"I've\", 'seen', 'things', 'you', 'people', \"wouldn't\", 'believe.']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_text.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red; font-size:1em; font-weight:bold\">L'exemple suivant permet de voir les points suivants :</span> \n",
    "\n",
    "- G√©n√©ration d'une liste de nombre de 10 Millions contenues entre 1 et 1000 \n",
    "- Cr√©ation d'un RDD Spark √† l'aide de la fontion parrallelize()\n",
    "- Transformer les valeurs de la liste en Tuples pour pouvoir cr√©er un DataFrame spark. \n",
    "- Cr√©ation du DataFrame Spark √† l'aide de la fonction createDataFrame. \n",
    "\n",
    "Pourquoi devons nous transformer les valeurs de notre liste en Tuple pour cr√©er un DataFrame Spark ? \n",
    "\n",
    "La raison est due √† la structure des donn√©es dans Spark. G√©n√©ralement les donn√©es sont d√©finies sous forme de tuple ou chaque tuple repr√©sente une colonne. \n",
    "\n",
    "liste = [1,2,3,4] --> Spark ne saura pas l'interpr√©ter sous forme de lignes et des colonnes. Il est n√©cessaire de convertir en tuple sous forme de (1,), (2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number = 10_000_000\n",
    "rdd_numbers = sc.parallelize(range(number)).map(lambda x: random.randint(1 , 1000))\n",
    "df = spark.createDataFrame(rdd_numbers.map(lambda x: (x,)))\n",
    "display(df.show(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red; font-size:1em; font-weight:bold\">Requ√™ter nos donn√©es grace √† SparkSql</span> \n",
    "\n",
    "Executer des requ√™tes sql sur un dataframe SPARK n√©cessite de cr√©er une vue d'une table en m√©moire. aveec la fonctione .createOrReplaceTempView pour le charger en m√©moire. A la suite de quoi nous pouvons ex√©cuter nos requ√™tes sql. \n",
    "\n",
    "- La vue permet de requ√™ter les donn√©es de mani√®res plus optimale. \n",
    "- Attention : Si la vue existe d√©ja, nous ne pouvons pas la r√©cr√©er. Il faudrait alors soit la supprimer ou utiliser une clause if not exits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.createTempView(\"my_view\") # premi√®re option \n",
    "df.createOrReplaceTempView(\"my_view\") # Deuxi√®me option \n",
    "result = spark.sql(\"select * from my_view where _1 = 994\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red; font-size:1em; font-weight:bold\">Manipuler nos DataFrames</span> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'_1'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Premi√®re fa√ßon d'acc√©der √† nos colonne simplement en les r√©f√©ren√ßant entre [\"\"]\n",
    "df[\"_1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `functions` de PySpark\n",
    "Acc√©der aux colonnes grace au module de PySpark sql qui s'appel functions. \n",
    "En revanche cette m√©thode ne fonction qu'√† l'int√©rieur des commandes sql. Raison pour laquelle on l'utilise dans la m√©thode select. \n",
    "Fonctions du module `functions` de PySpark\n",
    "\n",
    "Le module **`functions`** de PySpark propose une large gamme de fonctions pour effectuer des manipulations et des agr√©gations sur les colonnes d'un DataFrame. Voici quelques-unes des fonctions les plus couramment utilis√©es :\n",
    "\n",
    "### R√©sum√© des fonctions de `functions` de PySpark :\n",
    "\n",
    "- **S√©lection** : `col()`, `alias()`, `when()`, `lit()`, `cast()`.\n",
    "- **Agr√©gation** : `avg()`, `sum()`, `min()`, `max()`, `count()`, `countDistinct()`, `stddev()`, `variance()`, `first()`, `last()`.\n",
    "- **Fen√™tre** : `row_number()`, `rank()`, `dense_rank()`, `ntile()`, `lag()`, `lead()`.\n",
    "- **Math√©matiques** : `abs()`, `sqrt()`, `round()`, `exp()`, `log()`, `pow()`.\n",
    "- **Cha√Ænes** : `length()`, `trim()`, `regexp_extract()`, `regexp_replace()`.\n",
    "- **Dates** : `current_date()`, `current_timestamp()`, `date_format()`, `to_date()`, `year()`, `month()`, `dayofmonth()`.\n",
    "- **Nulles** : `isNull()`, `isNotNull()`, `coalesce()`.\n",
    "- **Autres transformations** : `collect_list()`, `collect_set()`, `concat_ws()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: bigint]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as f \n",
    "result.select(f.col(\"_1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[avg(_1): double]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.select(f.avg(\"_1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Action` de PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1=23), Row(_1=42)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, _1: string]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>10000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>500.3427784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>288.67221665815606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary                  _1\n",
       "0   count            10000000\n",
       "1    mean         500.3427784\n",
       "2  stddev  288.67221665815606\n",
       "3     min                   1\n",
       "4     max                1000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red; font-size:1em; font-weight:bold\">Utiliser Limit pour r√©cup√©rer qu'un nombre limiter de donn√©es</span> \n",
    "\n",
    "Lors de la manipulation des tr√®s gros volumes de donn√©es il faut veiller √† ne pas faire des op√©rations qui peuvent saturer la m√©moire. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>442.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>261.8659962652654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary                 _1\n",
       "0   count                  5\n",
       "1    mean              442.4\n",
       "2  stddev  261.8659962652654\n",
       "3     min                145\n",
       "4     max                756"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(5).describe().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Sauvergarder nos r√©sultat`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La m√©thode .write.mode(\"overwrite\").csv()\n",
    "\n",
    "Nous pr√©f√©rons cette m√©thode dans le cas ou notre DataFrame est tr√®s grand. √ßa permet de : \n",
    "\n",
    "- Gagner en performance : plus rapide d'√©crire les partitions que tout fusionner.\n",
    "- Eviter les probl√®mes de m√©moires. \n",
    "- Quand les donn√©es sont  stock√©s sur  : HDFS, S3, Google Cloud Storage. \n",
    "\n",
    "Nous utiliserons cette m√©thodes si nous voulons garder les partitions de nos donn√©es. Ou si nous souhaitons les partitionner selon des crit√®res \n",
    "Exemple : Ann√©es et Mois. \n",
    "\n",
    "df.write.partitionBy(\"ann√©e\", \"mois\").csv(\"hdfs://chemin_output\", header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").csv(\"./home/jovyan/work/monDfIssueDePySpark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La m√©thode .coalesce(1).write.mode(\"overwrite\").csv\n",
    "\n",
    "Cette m√©thode est √† √©viter lorsque le jeu de donn√©es est extr√™mement lourd. En effet, elle n'est pas efficace et tr√®s gourmande en m√©moire. \n",
    "A utiliser uniquement si nous voulons avoir un seul jeu de donn√©es xlsx ou un pandas. \n",
    "\n",
    "\n",
    "- En comparaison le m√™me df de 10 000 000 d'enregistrement est enregistr√© en 3.4s en multipart contre 21.8s en une seule partie et 3.3 en Parquet en plusiuers partie et 20.9 en une seule partie. \n",
    "\n",
    "√ßa montre que le mode parquet reste quand m√™me le mode le plus efficace √† l'export. Mais tout d√©pend de notre besoin final et l'usage qui sera fait de la donn√©e une fois export√©e. ¬µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.coalesce(1).write.mode(\"overwrite\").csv(\"./home/jovyan/work/monDfEnUnSeulFichier\", header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les fichier Parquet \n",
    "üöÄ Avantages du format Parquet :\n",
    "‚úÖ Stockage optimis√© & compression :\n",
    "\n",
    "Parquet stocke les donn√©es en colonnes et applique une compression efficace.\n",
    "R√©sultat : Fichiers plus petits (jusqu‚Äô√† 75% d‚Äô√©conomie compar√© √† CSV).\n",
    "‚úÖ Lecture rapide & s√©lective :\n",
    "\n",
    "Spark ne charge que les colonnes utilis√©es dans une requ√™te, ce qui acc√©l√®re l'analyse.\n",
    "Ex : avec CSV, Spark doit lire tout le fichier m√™me si on ne veut qu'une colonne.\n",
    "‚úÖ G√®re les types de donn√©es & les sch√©mas :\n",
    "\n",
    "Parquet conserve les types natifs (int, float, date, etc.), contrairement √† CSV o√π tout est en string.\n",
    "‚úÖ Meilleur pour les traitements distribu√©s :\n",
    "\n",
    "Supporte le partitionnement et l'indexation des fichiers ‚Üí booste la performance.\n",
    "Fonctionne super bien avec Hive, AWS Athena, Google BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En plusieurs fichiers \n",
    "df.write.mode(\"overwrite\").parquet(\"./home/jovyan/work/MonFichierParquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En un seul fichier \n",
    "df.coalesce(1).write.mode(\"overwrite\").parquet(\"./home/jovyan/work/MonFichierParquetEnUnSeulFichier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
